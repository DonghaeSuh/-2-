{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1ba282-fbb5-4812-af29-20c03784f0f1",
   "metadata": {},
   "source": [
    "## tensorflow.keras.preprocessing.text 모듈의 Tokenizer 클래스 사용 \n",
    "### => 텍스트를 단어기반으로 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54c772-f9d9-4c19-a654-b846c7fd45c7",
   "metadata": {},
   "source": [
    "## fit_on_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3b89e5-5c80-475e-9cd3-806f7dd15e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'y': 2, 'g': 1, 'o': 2, ' ': 2, 'd': 1, 'l': 2, 'e': 2, 'm': 2, 'v': 2, 'I': 2, 'c': 1, 'a': 1, 't': 1})\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "  'I love my dog',\n",
    "  'I love my cat'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_sequences(sentences)\n",
    "index_docs = tokenizer.index_docs\n",
    "print(index_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c301f94d-7f24-4666-b995-190d20bdf3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y', 'g', 'o', ' ', 'd', 'l', 'e', 'm', 'v', 'I'}\n"
     ]
    }
   ],
   "source": [
    "seq=set('I love my dog')\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be361c1-e5fc-4cea-8a9c-90853a8a3e06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## fit_on_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab612f39-ce5c-474d-bb49-8fd806784558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "  'I love my dog',\n",
    "  'I love my cat',\n",
    "  'You love my dog!'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d3ca0-3844-43cf-a03f-91146a7228d0",
   "metadata": {},
   "source": [
    "## text_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86961067-a536-4a4c-88c2-5bca12f3a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sequences generated from text are :  [[2, 1, 3], [2, 1], [4, 1], [5, 6]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# define 4 documents\n",
    "docs =['Machine Learning Knowledge',\n",
    "       'Machine Learning and Deep Learning',\n",
    "       'Deep Learning',\n",
    "       'Artificial Intelligence']\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(test_text)\n",
    "\n",
    "sequences = t.texts_to_sequences(test_text)\n",
    "\n",
    "print(\"The sequences generated from text are : \",sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299bf79-def5-4498-8e80-812c7bf6198d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## texts_to_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0cbfee3-ce06-4033-9ffa-93fb94db38e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning': 1, 'marvellous': 2, 'machine': 3, 'notorious': 4, 'natural': 5, 'language': 6, 'processing': 7, 'amazing': 8, 'artificial': 9, 'intelligence': 10, 'dazzling': 11, 'deep': 12, 'champion': 13, 'computer': 14, 'vision': 15}\n",
      "[[0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "# define 5 documents\n",
    "docs = ['Marvellous Machine Learning Marvellous Machine Learning',\n",
    "        'Amazing Artificial Intelligence',\n",
    "        'Dazzling Deep Learning',\n",
    "        'Champion Computer Vision',\n",
    "        'Notorious Natural Language Processing Notorious Natural Language Processing']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "\n",
    "t.fit_on_texts(docs)\n",
    "print(t.word_index)\n",
    "\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='binary')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8c1fc-a788-4413-a543-44e10546d700",
   "metadata": {},
   "source": [
    "## sequences_to_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ea07448-39b5-4f47-9dee-dc181b67f8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, 4], [2, 1, 5, 3, 1], [3, 1], [6, 7]]\n",
      "[[0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as t\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# define 4 documents\n",
    "docs =['Machine Learning Knowledge',\n",
    "       'Machine Learning and Deep Learning',\n",
    "       'Deep Learning',\n",
    "       'Artificial Intelligence']\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "\n",
    "t.fit_on_texts(docs)  # 텍스트들에 등장한 단어들에 정수를 분여\n",
    "\n",
    "sequences = t.texts_to_sequences(docs) # 문서에 등장한 단어들을 정수들로 표현\n",
    "print(sequences)\n",
    "\n",
    "encoded_docs = t.sequences_to_matrix(sequences, mode='binary') \n",
    "# 8개의 단어가 등장했다. 등장한 단어는1 아닌 단어는 0\n",
    "# mode = one of \"binary\", \"count\", \"tfidf\", \"freq\"\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc97c9-adc9-412a-abce-d7732f8feab0",
   "metadata": {},
   "source": [
    "## sequences_to_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c515e714-edf9-4abb-afc4-546897a448c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, 4], [2, 1, 5, 3, 1], [3, 1], [6, 7]]\n",
      "['machine learning knowledge', 'machine learning and deep learning', 'deep learning', 'artificial intelligence']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as t\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# define 4 documents\n",
    "docs =['Machine Learning Knowledge',\n",
    "       'Machine Learning and Deep Learning',\n",
    "       'Deep Learning',\n",
    "       'Artificial Intelligence']\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "\n",
    "t.fit_on_texts(docs)  # 텍스트들에 등장한 단어들에 정수를 분여\n",
    "\n",
    "sequences = t.texts_to_sequences(docs) # 문서에 등장한 단어들을 정수들로 표현\n",
    "print(sequences)\n",
    "\n",
    "\n",
    "\n",
    "seq_to_txt = t.sequences_to_texts(sequences) # 정수로 된 sequence를 text로 표현\n",
    "print(seq_to_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff92d0d-e76b-4728-b153-04c12806c266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
