{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eaa3fea-0d1f-4c72-b65f-c17a9035dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192040bf-90ae-4bec-a7d4-025fad5251de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.62.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.27.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (58.0.4)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.22.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gypsi\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.4)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d22728-7564-4a37-baeb-c93999ea07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\"\n",
    "\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "197f5317-13bd-4ee3-927f-e8e1662cb4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'computer', 'science', ',', 'information', 'engineering', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokenized_sentence = [token.text for token in doc]\n",
    "print(word_tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9dbec4-a3d6-42b7-891a-799b473cc0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenized_list = [sent.text for sent in doc.sents]\n",
    "print(sentence_tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca6324-1b85-465a-b997-5715f5cc4254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
