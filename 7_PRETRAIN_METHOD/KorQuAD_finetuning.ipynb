{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonghaeSuh/NLP_tensorflow2/blob/main/7_PRETRAIN_METHOD/KorQuAD_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlr9pci30HED"
      },
      "source": [
        "### 허깅페이스 transformers 라이브러리 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duaQf_Ln0BVL",
        "outputId": "a5c6e84b-7b6a-4ac0-a549-f6cfc00e3f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.97)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.53)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.64.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers==3.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4XOgvIn0P9A",
        "outputId": "3e2c07be-9597-4e29-fa77-73f7568181cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=bfca24ca3a8c70e12df41acc1d2d04f4bea2e1497ea592fcff5a7517d9b571ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GzVSAZNDzssX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib\n",
        "import wget\n",
        "\n",
        "MAX_LEN = 384\n",
        "EPOCHS = 2   # 원래는 3으로 할 예정이였지만 , colab 기본 사양 런타임 초과로 인해 2로 줄였다.\n",
        "VERBOSE = 2\n",
        "BATCH_SIZE = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkgBPGy60NBw",
        "outputId": "9c65a866-247e-4fdd-ce0d-03351245097d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/BERT'\n",
            "/content/drive/MyDrive/BERT\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BnlehZwEz-rz"
      },
      "outputs": [],
      "source": [
        "DATA_OUT_PATH = './data_out'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hPjdcJ5W0isu"
      },
      "outputs": [],
      "source": [
        "SEED_NUM = 1234\n",
        "tf.random.set_seed(SEED_NUM)\n",
        "np.random.seed(SEED_NUM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "iTZv-HZ60i8X"
      },
      "outputs": [],
      "source": [
        "# Save the slow pretrained tokenizer\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", lowercase=False)\n",
        "save_path = \"bert-base-multilingual-cased/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "tokenizer = BertWordPieceTokenizer(\"bert-base-multilingual-cased/vocab.txt\", lowercase=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NuO1PbBa0qtY"
      },
      "outputs": [],
      "source": [
        "train_data_url = \"https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\"\n",
        "train_path = keras.utils.get_file(\"train.json\", train_data_url)\n",
        "eval_data_url = \"https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\"\n",
        "eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x6VUaMfLnHfq",
        "outputId": "d2c64bbb-0f62-421e-9855-e91d15d28fd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwith open(train_path) as f:\\n  raw_train_data=json.load(f)\\n\\nraw_train_data\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "'''\n",
        "with open(train_path) as f:\n",
        "  raw_train_data=json.load(f)\n",
        "\n",
        "raw_train_data\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8AJpA2eq0rFY",
        "outputId": "10da1316-5885-4297-af3c-83d3e658ebf1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./bert-base-multilingual-cased//bert-base-multilingual-cased-config.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "wget.download('https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json', out='./bert-base-multilingual-cased/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ./bert-base-multilingual-cased/bert-base-multilingual-cased-config.json ./bert-base-multilingual-cased/config.json"
      ],
      "metadata": {
        "id": "n6OtWXIh3hkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99bffcd-c579-4aa4-cb7b-3f327ac879d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat './bert-base-multilingual-cased/bert-base-multilingual-cased-config.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P ./bert-base-multilingual-cased/ https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-tf_model.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V7Dfcrb3jOf",
        "outputId": "63980569-c9ec-4f51-a1c1-5301940c0eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-12 06:08:25--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-tf_model.h5\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.70.62\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.70.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1083389348 (1.0G) [binary/octet-stream]\n",
            "Saving to: ‘./bert-base-multilingual-cased/bert-base-multilingual-cased-tf_model.h5’\n",
            "\n",
            "bert-base-multiling 100%[===================>]   1.01G  42.6MB/s    in 25s     \n",
            "\n",
            "2022-09-12 06:08:50 (41.0 MB/s) - ‘./bert-base-multilingual-cased/bert-base-multilingual-cased-tf_model.h5’ saved [1083389348/1083389348]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ./bert-base-multilingual-cased/bert-base-multilingual-cased-tf_model.h5 ./bert-base-multilingual-cased/tf_model.h5"
      ],
      "metadata": {
        "id": "6YoqyFWx3jin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SquadExample:\n",
        "    def __init__(self, question, context, start_char_idx, answer_text):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.skip = False\n",
        "\n",
        "    def preprocess(self):\n",
        "        context = self.context\n",
        "        question = self.question\n",
        "        answer_text = self.answer_text\n",
        "        start_char_idx = self.start_char_idx\n",
        "\n",
        "        # Clean context, answer and question\n",
        "        context = \" \".join(str(context).split())\n",
        "        question = \" \".join(str(question).split())\n",
        "        answer = \" \".join(str(answer_text).split())\n",
        "\n",
        "        # Find end character index of answer in context\n",
        "        end_char_idx = start_char_idx + len(answer)\n",
        "        if end_char_idx >= len(context):\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Mark the character indexes in context that are in answer\n",
        "        is_char_in_ans = [0] * len(context)\n",
        "        for idx in range(start_char_idx, end_char_idx):\n",
        "            is_char_in_ans[idx] = 1\n",
        "\n",
        "        # Tokenize context\n",
        "        tokenized_context = tokenizer.encode(context)\n",
        "\n",
        "        # Find tokens that were created from answer characters  \n",
        "        ans_token_idx = []\n",
        "        for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
        "            if sum(is_char_in_ans[start:end]) > 0:\n",
        "                ans_token_idx.append(idx)\n",
        "\n",
        "        if len(ans_token_idx) == 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Find start and end token index for tokens from answer\n",
        "        start_token_idx = ans_token_idx[0]\n",
        "        end_token_idx = ans_token_idx[-1]\n",
        "\n",
        "        # Tokenize question\n",
        "        tokenized_question = tokenizer.encode(question)\n",
        "\n",
        "        # Create inputs\n",
        "        # 텍스트를 .encode(text)하면 나오는 sequence는 맨 앞에 [CLS]와 맨 뒤에 [SEP] 토큰이 붙어서 나온다. \n",
        "        # 그러므로, 두개의 encode된 ids를 이어 붙이기 위해서는 뒷 sequence의 [CLS]를 없에주어야 한다. \n",
        "        # 그래서 [1:]을 통해 0번째가 아닌 1번째 index부터 맨 뒤까지 슬라이싱한다.\n",
        "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]   \n",
        "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(    \n",
        "            tokenized_question.ids[1:] \n",
        "        )                                                                  \n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Pad and create attention masks.\n",
        "        # Skip if truncation is needed\n",
        "        padding_length = MAX_LEN - len(input_ids)\n",
        "        if padding_length > 0:  # pad\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:  # skip\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.context_token_to_char = tokenized_context.offsets\n",
        "\n",
        "\n",
        "def create_squad_examples(raw_data):\n",
        "    squad_examples = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                squad_eg = SquadExample(\n",
        "                    question, context, start_char_idx, answer_text\n",
        "                )\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "    return squad_examples\n",
        "\n",
        "\n",
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],  # squad_example 개수 x MAX_LEN\n",
        "        dataset_dict[\"token_type_ids\"], # squad_example 개수 x MAX_LEN\n",
        "        dataset_dict[\"attention_mask\"], # squad_example 개수 x MAX_LEN\n",
        "    ]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]] # 각각 squad_example 개수 x 1\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "vURRWxRs38MP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(train_path) as f:\n",
        "    raw_train_data = json.load(f)\n",
        "\n",
        "with open(eval_path) as f:\n",
        "    raw_eval_data = json.load(f)\n",
        "\n",
        "\n",
        "train_squad_examples = create_squad_examples(raw_train_data)\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\n",
        "\n",
        "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")"
      ],
      "metadata": {
        "id": "ICmp5rgdPzZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a814326b-5a3f-4938-c4d9-dbd2628d67ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60407 training points created.\n",
            "5774 evaluation points created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TFBERTQuestionAnswering(tf.keras.Model):\n",
        "    def __init__(self, model_name, dir_path, num_class):\n",
        "        super(TFBERTQuestionAnswering, self).__init__()\n",
        "        \n",
        "        self.encoder = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
        "        self.start_logit = tf.keras.layers.Dense(num_class, name=\"start_logit\", use_bias=False)\n",
        "        self.end_logit = tf.keras.layers.Dense(num_class, name=\"end_logit\", use_bias=False)\n",
        "        self.flatten = tf.keras.layers.Flatten() \n",
        "        self.softmax = tf.keras.layers.Activation(tf.keras.activations.softmax)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        input_ids, token_type_ids, attention_mask = inputs\n",
        "        embedding = self.encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
        "        start_logits = self.start_logit(embedding)\n",
        "        start_logits = self.flatten(start_logits)\n",
        "        \n",
        "        end_logits = self.end_logit(embedding)\n",
        "        end_logits = self.flatten(end_logits)\n",
        "        \n",
        "        start_probs = self.softmax(start_logits)\n",
        "        end_probs = self.softmax(end_logits)\n",
        "    \n",
        "        return start_probs, end_probs"
      ],
      "metadata": {
        "id": "4kJewGwMP0Lr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "korquad_model = TFBERTQuestionAnswering(model_name='./bert-base-multilingual-cased/',dir_path='bert_ckpt', num_class=1)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
      ],
      "metadata": {
        "id": "8n2acgPGP3Nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86256330-ca46-4c10-8a75-fdc5e45bf53b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers.modeling_tf_utils:Some weights of the model checkpoint at ./bert-base-multilingual-cased/ were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:transformers.modeling_tf_utils:All the weights of TFBertModel were initialized from the model checkpoint at ./bert-base-multilingual-cased/.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalized_answer(s):    \n",
        "    def remove_(text):\n",
        "        ''' 불필요한 기호 제거 '''\n",
        "        text = re.sub(\"'\", \" \", text)\n",
        "        text = re.sub('\"', \" \", text)\n",
        "        text = re.sub('《', \" \", text)\n",
        "        text = re.sub('》', \" \", text)\n",
        "        text = re.sub('<', \" \", text)\n",
        "        text = re.sub('>', \" \", text) \n",
        "        text = re.sub('〈', \" \", text)\n",
        "        text = re.sub('〉', \" \", text)   \n",
        "        text = re.sub(\"\\(\", \" \", text)\n",
        "        text = re.sub(\"\\)\", \" \", text)\n",
        "        text = re.sub(\"‘\", \" \", text)\n",
        "        text = re.sub(\"’\", \" \", text)      \n",
        "        return text\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(remove_(s))))"
      ],
      "metadata": {
        "id": "_3sH6gyrP3nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExactMatch(keras.callbacks.Callback):\n",
        "    def __init__(self, x_eval, y_eval):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "        count = 0\n",
        "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "            squad_eg = eval_examples_no_skip[idx]\n",
        "            offsets = squad_eg.context_token_to_char\n",
        "            start = np.argmax(start)\n",
        "            end = np.argmax(end)\n",
        "            if start >= len(offsets):   # 기본적으로 offsets의 길이는 context를 BertWordPieceTokenizer를 이용해 토큰 분리한 개수 + 2 \n",
        "                continue               #( 맨 앞과 끝에 (0,0)이 존재 : [CLS]와[SEP]용도) \n",
        "                                      # => start==len(offsets)이면 (0,0)이 정답의 start라는 것이여서 안됨.=> 범위에 포함\n",
        "            pred_char_start = offsets[start][0] #기존 context에서 token의 시작 위치 ind          \n",
        "            if end < len(offsets):  # 마지막 (0,0)을 포함하지 않는다.\n",
        "                pred_char_end = offsets[end][1] # 기존 context에서 정답 마지막 token의 위치 + 1  : +1인 이유는 파이썬 열린구간 범위계산은 \n",
        "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]     # (a,b) => a<= x < b 이므로 맨 뒤 하나를 포함시키지 않아서\n",
        "            else:                                                              #  offsets에서도 b값은 +1 되어있다.\n",
        "                pred_ans = squad_eg.context[pred_char_start:]    # end < len(offsets) 이 아닌경우는 맨 마지막 (0,0)을  포함하는 경우이기 때문에 \n",
        "                                                                  # 시작지점부터 context 끝까지 모두 정답으로 판단한다.\n",
        "            normalized_pred_ans = normalized_answer(pred_ans)\n",
        "            normalized_true_ans = normalized_answer(squad_eg.answer_text)\n",
        "            if normalized_pred_ans in normalized_true_ans:\n",
        "                count += 1\n",
        "        acc = count / len(self.y_eval[0])   # 지표 : 정확도(accuracy)   정답/실제\n",
        "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")"
      ],
      "metadata": {
        "id": "f2uplQItP5pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exact_match_callback = ExactMatch(x_eval, y_eval)"
      ],
      "metadata": {
        "id": "rK7eZHHtP9cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "korquad_model.compile(optimizer=optimizer, loss=[loss,loss])"
      ],
      "metadata": {
        "id": "NgLGS05-P929"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"tf2_bert_korquad\"\n",
        "\n",
        "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)"
      ],
      "metadata": {
        "id": "PkJ56laTP_B1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d5e098-0cbf-44dc-c824-8a969bdad2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data_out/tf2_bert_korquad -- Folder already exists \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history = korquad_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=EPOCHS,  # For demonstration, 3 epochs are recommended\n",
        "    verbose=VERBOSE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[exact_match_callback, cp_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "TJ1C_w4WQAWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0516e174-4aeb-40ed-fa0d-b6d2fe400a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch=1, exact match score=0.72\n",
            "2941/2941 - 4257s - loss: 1.5218 - output_1_loss: 0.6996 - output_2_loss: 0.8222 - 4257s/epoch - 1s/step\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch=2, exact match score=0.73\n",
            "2941/2941 - 4232s - loss: 1.0320 - output_1_loss: 0.4674 - output_2_loss: 0.5646 - 4232s/epoch - 1s/step\n",
            "Epoch 3/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history)"
      ],
      "metadata": {
        "id": "nbyw6jo2QB-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string, string_1, string_2):\n",
        "    # loss \n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history[string_1])\n",
        "    plt.plot(history.history[string_2])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, string_1, string_2])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-y1_7RR5QGTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs(history, 'loss', 'output_1_loss', 'output_2_loss')"
      ],
      "metadata": {
        "id": "hqlZe37fQDh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "ASHCDyrPiYFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfR0qWg00u45"
      },
      "outputs": [],
      "source": [
        "a='나는 nlp를 공부하고 있는 사람이다.'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxNF7ET1ihuJ",
        "outputId": "8084bbc3-e674-49fd-8123-b9117423f914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(a).tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtELBFasijnI",
        "outputId": "1a7ca4d2-0965-4a00-a045-9b72e83d0930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " '나는',\n",
              " 'nl',\n",
              " '##p',\n",
              " '##를',\n",
              " '공',\n",
              " '##부',\n",
              " '##하고',\n",
              " '있는',\n",
              " '사람이',\n",
              " '##다',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(a).offsets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv-rcwTlizd_",
        "outputId": "66e97b99-6df4-4e1a-817b-9cc1547822b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0),\n",
              " (0, 2),\n",
              " (3, 5),\n",
              " (5, 6),\n",
              " (6, 7),\n",
              " (8, 9),\n",
              " (9, 10),\n",
              " (10, 12),\n",
              " (13, 15),\n",
              " (16, 19),\n",
              " (19, 20),\n",
              " (20, 21),\n",
              " (0, 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(a).ids"
      ],
      "metadata": {
        "id": "NNl4xDK8i7GZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c245b844-853e-478c-c2bb-444c7d785e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 100585,\n",
              " 21517,\n",
              " 10410,\n",
              " 11513,\n",
              " 8896,\n",
              " 14646,\n",
              " 12453,\n",
              " 13767,\n",
              " 97802,\n",
              " 11903,\n",
              " 119,\n",
              " 102]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JuItwlXB2pwg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "1wWjPKM-Ybm-mzYaVE93MusRFaQrobbqo",
      "authorship_tag": "ABX9TyO00vkKXqMJ9i3vPf2by5vs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}