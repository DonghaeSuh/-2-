{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kRv5Qzgr-Sf8EFMs1jByPPkpD_fWPeEM",
      "authorship_tag": "ABX9TyPEQ+46jtbzGrkJwn2ktEoq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonghaeSuh/NLP_tensorflow2/blob/main/7_PRETRAIN_METHOD/GPT2/GPT2_KorNLI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 여러 도구들 install"
      ],
      "metadata": {
        "id": "YEqooy1pfZDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece==0.1.85"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfZyTvjNfVN8",
        "outputId": "1f8114ca-8fa3-4bfb-ef44-c2970bf58d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 32.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gluonnlp==0.9.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAuVhQFefdxp",
        "outputId": "3fd40149-0156-4aff-da70-e77f782b76bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gluonnlp==0.9.1\n",
            "  Downloading gluonnlp-0.9.1.tar.gz (252 kB)\n",
            "\u001b[K     |████████████████████████████████| 252 kB 31.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.9.1) (1.21.6)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.9.1) (0.29.32)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.9.1) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp==0.9.1) (3.0.9)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp37-cp37m-linux_x86_64.whl size=472292 sha256=e49cc2f07c4a2a4ceea2859753355d2dda0ef0af36d10b559e8841b535a0cd44\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/dd/b2/c023f6c9c83fb46b10f62f77ea526c4dad6913b967941bbe99\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mxnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fUgchyKfepR",
        "outputId": "1485f45e-0b88-4fd6-b0e5-bdf4db2d0593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1 MB 1.8 MB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.21.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==3.0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8wmYpKWfgC5",
        "outputId": "a437d814-c426-43b5-ea91-0da2cffae8c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 31.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.85)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 56.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 63.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2022.9.24)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=38d1f34653c7fcbc2966e0a0d1ecaf3476ac286ad370750af8f16e6d570d26ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 시작"
      ],
      "metadata": {
        "id": "_Fh4Zi4z2drV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from transformers import TFGPT2Model\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "nwUVC6nA2dJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED_NUM = 1234\n",
        "tf.random.set_seed(SEED_NUM)\n",
        "np.random.seed(SEED_NUM)"
      ],
      "metadata": {
        "id": "eQI5ArzV2icf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/GPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVcGUgwc27h2",
        "outputId": "3789e5a4-f197-4c8d-ad7d-69ed2a24162d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
        "\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token=None,\n",
        "                                               sep_token='<unused0>',\n",
        "                                               cls_token=None,\n",
        "                                               unknown_token='<unk>',\n",
        "                                               padding_token='<pad>',\n",
        "                                               bos_token='<s>',\n",
        "                                               eos_token='</s>')"
      ],
      "metadata": {
        "id": "14pYL3rW2j2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 2 # colab 기본요금제 제한으로 인한 Epoch 수 줄임 (원래 3) \n",
        "SENT_MAX_LEN = 31\n",
        "\n",
        "DATA_IN_PATH = './data_in'\n",
        "DATA_OUT_PATH = \"./data_out\""
      ],
      "metadata": {
        "id": "bpZ5u93g2lMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Train dataset\n",
        "\n",
        "TRAIN_SNLI_DF = os.path.join(DATA_IN_PATH, 'KorNLI', 'snli_1.0_train.ko.tsv')\n",
        "TRAIN_XNLI_DF = os.path.join(DATA_IN_PATH, 'KorNLI', 'multinli.train.ko.tsv')\n",
        "DEV_XNLI_DF = os.path.join(DATA_IN_PATH, 'KorNLI', 'xnli.dev.ko.tsv')\n",
        "\n",
        "train_data_snli = pd.read_csv(TRAIN_SNLI_DF, header=0, delimiter='\\t', quoting=3)\n",
        "train_data_xnli = pd.read_csv(TRAIN_XNLI_DF, header=0, delimiter='\\t', quoting=3)\n",
        "dev_data_xnli = pd.read_csv(DEV_XNLI_DF, header=0, delimiter='\\t', quoting=3)\n",
        "\n",
        "train_data_snli_xnli = train_data_snli.append(train_data_xnli)\n",
        "train_data_snli_xnli = train_data_snli_xnli.dropna()\n",
        "train_data_snli_xnli = train_data_snli_xnli.reset_index()\n",
        "\n",
        "dev_data_xnli = dev_data_xnli.dropna()\n",
        "\n",
        "print(\"Total # dataset: train - {}, dev - {}\".format(len(train_data_snli_xnli), len(dev_data_xnli)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI5rmtOZ2mLt",
        "outputId": "66151a22-f530-4c21-e324-1f404d5d1d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total # dataset: train - 942808, dev - 2490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저를 제외하고는 5장에서 처리한 방식과 유사하게 접근\n",
        "def clean_text(sent):\n",
        "    sent_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \" \", sent)\n",
        "    return sent_clean\n",
        "\n",
        "train_data_sents = []\n",
        "\n",
        "for train_sent_1, train_sent_2 in train_data_snli_xnli[['sentence1', 'sentence2']].values:\n",
        "    train_tokenized_sent_1 = vocab[tokenizer(clean_text(train_sent_1))]\n",
        "    train_tokenized_sent_2 = vocab[tokenizer(clean_text(train_sent_2))]\n",
        "\n",
        "    tokens = [vocab[vocab.bos_token]] \n",
        "    tokens += pad_sequences([train_tokenized_sent_1], \n",
        "                            SENT_MAX_LEN, \n",
        "                            value=vocab[vocab.padding_token], \n",
        "                            padding='post').tolist()[0] \n",
        "    tokens += [vocab[vocab.sep_token]]  \n",
        "    tokens += pad_sequences([train_tokenized_sent_2], \n",
        "                            SENT_MAX_LEN, \n",
        "                            value=vocab[vocab.padding_token], \n",
        "                            padding='post').tolist()[0] \n",
        "    tokens += [vocab[vocab.eos_token]]\n",
        "\n",
        "    train_data_sents.append(tokens)    \n",
        "\n",
        "train_data_sents = np.array(train_data_sents, dtype=np.int64)"
      ],
      "metadata": {
        "id": "BdhhjjTg2zAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_data_sents = []\n",
        "\n",
        "for dev_sent_1, dev_sent_2 in dev_data_xnli[['sentence1', 'sentence2']].values:\n",
        "    dev_tokenized_sent_1 = vocab[tokenizer(clean_text(dev_sent_1))]\n",
        "    dev_tokenized_sent_2 = vocab[tokenizer(clean_text(dev_sent_2))]\n",
        "\n",
        "    tokens = [vocab[vocab.bos_token]] \n",
        "    tokens += pad_sequences([dev_tokenized_sent_1], \n",
        "                            SENT_MAX_LEN, \n",
        "                            value=vocab[vocab.padding_token], \n",
        "                            padding='post').tolist()[0] \n",
        "    tokens += [vocab[vocab.sep_token]]  \n",
        "    tokens += pad_sequences([dev_tokenized_sent_2], \n",
        "                            SENT_MAX_LEN, \n",
        "                            value=vocab[vocab.padding_token], \n",
        "                            padding='post').tolist()[0] \n",
        "    tokens += [vocab[vocab.eos_token]]\n",
        "\n",
        "    dev_data_sents.append(tokens)    \n",
        "\n",
        "dev_data_sents = np.array(dev_data_sents, dtype=np.int64)"
      ],
      "metadata": {
        "id": "C_UbmdPt3TmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label을 Netural, Contradiction, Entailment 에서 숫자 형으로 변경한다.\n",
        "label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
        "\n",
        "def convert_int(label):\n",
        "    num_label = label_dict[label]    \n",
        "    return num_label\n",
        "\n",
        "train_data_snli_xnli[\"gold_label_int\"] = train_data_snli_xnli[\"gold_label\"].apply(convert_int)\n",
        "train_data_labels = np.array(train_data_snli_xnli['gold_label_int'], dtype=int)\n",
        "\n",
        "dev_data_xnli[\"gold_label_int\"] = dev_data_xnli[\"gold_label\"].apply(convert_int)\n",
        "dev_data_labels = np.array(dev_data_xnli['gold_label_int'], dtype=int)\n",
        "\n",
        "print(\"# train labels: {}, #dev labels: {}\".format(len(train_data_labels), len(dev_data_labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFAJumrf3W_Q",
        "outputId": "05a5629a-37a5-4edf-f08f-bac2f36f30a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# train labels: 942808, #dev labels: 2490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TFGPT2Classifier(tf.keras.Model):\n",
        "    def __init__(self, dir_path, num_class):\n",
        "        super(TFGPT2Classifier, self).__init__()\n",
        "        \n",
        "        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\n",
        "        self.num_class = num_class\n",
        "        \n",
        "        self.dropout = tf.keras.layers.Dropout(self.gpt2.config.summary_first_dropout)\n",
        "        self.classifier = tf.keras.layers.Dense(self.num_class, \n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=self.gpt2.config.initializer_range), \n",
        "                                                name=\"classifier\")\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        outputs = self.gpt2(inputs)\n",
        "        pooled_output = outputs[0][:, -1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "YKtxXgeE3Y49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL_PATH = './gpt_ckpt'\n",
        "sim_model = TFGPT2Classifier(dir_path=BASE_MODEL_PATH, num_class=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE9pRDcu3bPN",
        "outputId": "d2504e0d-cba3-43a2-ab63-8e17cec3a648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers.modeling_tf_utils:All model checkpoint weights were used when initializing TFGPT2Model.\n",
            "\n",
            "WARNING:transformers.modeling_tf_utils:All the weights of TFGPT2Model were initialized from the model checkpoint at ./gpt_ckpt.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(6.25e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "sim_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "metadata": {
        "id": "jcXlS9rH3bn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"tf2_gpt_kornli\"\n",
        "\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
        "\n",
        "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "history = sim_model.fit(train_data_sents, train_data_labels, \n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        validation_data=(dev_data_sents, dev_data_labels),\n",
        "                        batch_size=BATCH_SIZE, \n",
        "                        callbacks=[earlystop_callback, cp_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4x0jKwR3csV",
        "outputId": "5f6a012c-d54c-44a1-9828-28a17df56d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data_out/tf2_gpt_kornli -- Folder already exists \n",
            "\n",
            "Epoch 1/2\n",
            " 6480/29463 [=====>........................] - ETA: 2:50:08 - loss: 0.7039 - accuracy: 0.6948"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vgi6Ktij8m2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs(history, 'accuracy')"
      ],
      "metadata": {
        "id": "ZYKCJF_g8o7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "id": "utDMFCh58pp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트"
      ],
      "metadata": {
        "id": "OOBlblcm8q2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Test dataset\n",
        "TEST_XNLI_DF = os.path.join(DATA_IN_PATH, 'KorNLI', 'xnli.test.ko.tsv')\n",
        "\n",
        "test_data_xnli = pd.read_csv(TEST_XNLI_DF, header=0, delimiter='\\t', quoting=3)"
      ],
      "metadata": {
        "id": "jvTNMjAC8rrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_data_xnli = test_data_xnli[:50] # for test\n",
        "\n",
        "test_data_sents = []\n",
        "\n",
        "for test_sent_1, test_sent_2 in test_data_xnli[['sentence1', 'sentence2']].values:\n",
        "    test_tokenized_sent_1 = vocab[tokenizer(clean_text(test_sent_1))]\n",
        "    test_tokenized_sent_2 = vocab[tokenizer(clean_text(test_sent_2))]\n",
        "\n",
        "    tokens = [vocab[vocab.bos_token]] \n",
        "    tokens += pad_sequences([test_tokenized_sent_1], \n",
        "                            SENT_MAX_LEN, \n",
        "                            value=vocab[vocab.padding_token], \n",
        "                            padding='post').tolist()[0] \n",
        "    tokens += [vocab[vocab.sep_token]]  \n",
        "    tokens += pad_sequences([test_tokenized_sent_2], \n",
        "                            SENT_MAX_LEN, \n",
        "                            value=vocab[vocab.padding_token], \n",
        "                            padding='post').tolist()[0] \n",
        "    tokens += [vocab[vocab.eos_token]]\n",
        "\n",
        "    test_data_sents.append(tokens)    \n",
        "\n",
        "test_data_sents = np.array(test_data_sents, dtype=np.int64)"
      ],
      "metadata": {
        "id": "wGkwVqwg8tOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_xnli[\"gold_label_int\"] = test_data_xnli[\"gold_label\"].apply(convert_int)\n",
        "test_data_labels = np.array(test_data_xnli['gold_label_int'], dtype=int)\n",
        "\n",
        "print(\"# sents: {}, # labels: {}\".format(len(test_data_sents), len(test_data_labels)))"
      ],
      "metadata": {
        "id": "XUQ7B-m_8u_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_model.load_weights(checkpoint_path)\n",
        "\n",
        "results = sim_model.evaluate(test_data_sents, test_data_labels, batch_size=1024)\n",
        "print(\"test loss, test acc: \", results)"
      ],
      "metadata": {
        "id": "3js9Nss78xY7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}