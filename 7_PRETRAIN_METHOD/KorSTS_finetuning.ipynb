{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonghaeSuh/NLP_tensorflow2/blob/main/7_PRETRAIN_METHOD/KorSTS_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqHJSiIjDnig"
      },
      "source": [
        "### 허깅페이스 트랜스포머 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7cVr2a2Z2f-",
        "outputId": "b626acca-adf6-4776-a1a1-2a72181bc2dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.8.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.97)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.53)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.6.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers==3.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cFtwzjfDaggk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O_ARgkjka5ZT"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\",\n",
        "                                          cache_dir='bert_ckpt',\n",
        "                                          do_lower_case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4G1inWdabMRL"
      },
      "outputs": [],
      "source": [
        "#random seed 고정\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# BASE PARAM\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "VALID_SPLIT = 0.2\n",
        "MAX_LEN = 28 * 2 \n",
        "\n",
        "DATA_IN_PATH = 'data_in'\n",
        "DATA_OUT_PATH = \"data_out\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4OUMp1wbQOq"
      },
      "source": [
        "\n",
        "### KorSTS Dataset\n",
        "Data from Kakaobrain: https://github.com/kakaobrain/KorNLUDatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJD7fyXAbWpS",
        "outputId": "a5564c49-a29d-4849-aa73-e7524b709358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/BERT'\n",
            "/content/drive/MyDrive/BERT\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpo3GwqsbN3C",
        "outputId": "3c1460ed-8717-4965-bd2d-3006472bd1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total # dataset: train - 5749, dev - 1500\n"
          ]
        }
      ],
      "source": [
        "# Load Train dataset\n",
        "\n",
        "TRAIN_STS_DF = os.path.join(DATA_IN_PATH, 'KorSTS', 'sts-train.tsv')\n",
        "DEV_STS_DF = os.path.join(DATA_IN_PATH, 'KorSTS', 'sts-dev.tsv')\n",
        "\n",
        "train_data = pd.read_csv(TRAIN_STS_DF, header=0, delimiter = '\\t', quoting = 3)\n",
        "dev_data = pd.read_csv(DEV_STS_DF, header=0, delimiter = '\\t', quoting = 3)\n",
        "\n",
        "print(\"Total # dataset: train - {}, dev - {}\".format(len(train_data), len(dev_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XMC4vsizbZlr"
      },
      "outputs": [],
      "source": [
        "# Bert Tokenizer\n",
        "\n",
        "# 참조: https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus\n",
        "\n",
        "def bert_tokenizer_v2(sent1, sent2, MAX_LEN):\n",
        "    \n",
        "    # For Two setenece input\n",
        "    \n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text = sent1,\n",
        "        text_pair = sent2,\n",
        "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "        max_length = MAX_LEN,           # Pad & truncate all sentences.\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True,   # Construct attn. masks.\n",
        "        truncation = True\n",
        "    )\n",
        "    \n",
        "    input_id = encoded_dict['input_ids']\n",
        "    attention_mask = encoded_dict['attention_mask'] # And its attention mask (simply differentiates padding from non-padding).\n",
        "    token_type_id = encoded_dict['token_type_ids'] # differentiate two sentences\n",
        "    \n",
        "    return input_id, attention_mask, token_type_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mwPoYzYWbovq"
      },
      "outputs": [],
      "source": [
        "# 토크나이저를 제외하고는 5장에서 처리한 방식과 유사하게 접근\n",
        "def clean_text(sent):\n",
        "    sent_clean = re.sub(\"[^a-zA-Z0-9ㄱ-ㅣ가-힣\\\\s]\", \" \", sent)\n",
        "    return sent_clean\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "data_labels = []\n",
        "\n",
        "\n",
        "for sent1, sent2, score in train_data[['sentence1', 'sentence2', 'score']].values:\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer_v2(clean_text(sent1), clean_text(sent2), MAX_LEN)\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        data_labels.append(score)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(sent1, sent2)\n",
        "        pass\n",
        "    \n",
        "train_input_ids = np.array(input_ids, dtype=int)\n",
        "train_attention_masks = np.array(attention_masks, dtype=int)\n",
        "train_type_ids = np.array(token_type_ids, dtype=int)\n",
        "train_inputs = (train_input_ids, train_attention_masks, train_type_ids)\n",
        "train_data_labels = np.array(data_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 검증(DEV) 데이터 전처리"
      ],
      "metadata": {
        "id": "gjlZE7f8ErR9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MD_Ool-4bqfK"
      },
      "outputs": [],
      "source": [
        "# 토크나이저를 제외하고는 5장에서 처리한 방식과 유사하게 접근\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "data_labels = []\n",
        "\n",
        "for sent1, sent2, score in dev_data[['sentence1', 'sentence2', 'score']].values:\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer_v2(clean_text(sent1), clean_text(sent2), MAX_LEN)\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        data_labels.append(score)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(sent1, sent2)\n",
        "        pass\n",
        "    \n",
        "dev_input_ids = np.array(input_ids, dtype=int)\n",
        "dev_attention_masks = np.array(attention_masks, dtype=int)\n",
        "dev_type_ids = np.array(token_type_ids, dtype=int)\n",
        "dev_inputs = (dev_input_ids, dev_attention_masks, dev_type_ids)\n",
        "dev_data_labels = np.array(data_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"# train labels: {}, #dev labels: {}\".format(len(train_data_labels), len(dev_data_labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKgNgEVYE5ZJ",
        "outputId": "d68972bd-2a05-4dba-eef9-867a595463d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# train labels: 5749, #dev labels: 1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TFBertRegressor(tf.keras.Model):\n",
        "    def __init__(self, model_name, dir_path, num_class):\n",
        "        super(TFBertRegressor, self).__init__()\n",
        "        \n",
        "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
        "        self.num_class = num_class\n",
        "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        self.regressor = tf.keras.layers.Dense(self.num_class, \n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
        "                                                name=\"regressor\")\n",
        "        \n",
        "        \n",
        "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
        "        \n",
        "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.regressor(pooled_output)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "my6Z1gMWE7qR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_model = TFBertRegressor(model_name='bert-base-multilingual-cased',\n",
        "                                  dir_path='bert_ckpt',\n",
        "                                  num_class=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbPL6yP-E-Ih",
        "outputId": "7e3f97ab-f668-45fb-c2bb-b4c893043ebd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers.modeling_tf_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:transformers.modeling_tf_utils:All the weights of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PearsonCorrelationMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name=\"pearson_correlation\", **kwargs):\n",
        "        super(PearsonCorrelationMetric, self).__init__(name=name, **kwargs)\n",
        "        self.y_true_list = []\n",
        "        self.y_pred_list = []\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.reshape(y_true, shape=[-1])\n",
        "        y_pred = tf.reshape(y_pred, shape=[-1])\n",
        "        self.y_true_list.append(y_true)\n",
        "        self.y_pred_list.append(y_pred)\n",
        "\n",
        "    def result(self):\n",
        "        y_true = tf.concat(self.y_true_list, -1)\n",
        "        y_pred = tf.concat(self.y_pred_list, -1)\n",
        "        pearson_correlation = self.pearson(y_true, y_pred)\n",
        "        \n",
        "        return pearson_correlation\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.y_true_list = []\n",
        "        self.y_pred_list = []\n",
        "        \n",
        "\n",
        "    def pearson(self, true, pred):\n",
        "        m_true = tf.reduce_mean(true)\n",
        "        m_pred = tf.reduce_mean(pred)\n",
        "        m_true, m_pred = true-m_true, pred-m_pred\n",
        "        num = tf.reduce_sum(tf.multiply(m_true, m_pred))\n",
        "        den = tf.sqrt(tf.multiply(tf.reduce_sum(tf.square(m_true)), tf.reduce_sum(tf.square(m_pred)))) + 1e-12\n",
        "        return num / den"
      ],
      "metadata": {
        "id": "byBf5PRBFAJw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "metric = PearsonCorrelationMetric()\n",
        "regression_model.compile(optimizer=optimizer, loss=loss, metrics=[metric], run_eagerly=True)"
      ],
      "metadata": {
        "id": "918ZFMP0FCDw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 진행하기\n",
        "model_name = \"tf2_BERT_KorSTS\"\n",
        "\n",
        "# overfitting을 막기 위한 ealrystop 추가\n",
        "earlystop_callback = EarlyStopping(monitor='val_pearson_correlation', min_delta=0.0001,patience=2,mode='max')\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
        "\n",
        "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_pearson_correlation', verbose=1, save_best_only=True, save_weights_only=True,mode='max')\n",
        "\n",
        "# 학습과 eval 시작\n",
        "history = regression_model.fit(train_inputs, train_data_labels, epochs=NUM_EPOCHS,\n",
        "            validation_data = (dev_inputs, dev_data_labels),\n",
        "            batch_size=BATCH_SIZE, callbacks=[earlystop_callback, cp_callback])\n",
        "\n",
        "#steps_for_epoch\n",
        "print(history.history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg-v6VfEFFfg",
        "outputId": "fcef7aa8-c13a-4b12-8073-4eca71c8caa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_out/tf2_BERT_KorSTS -- Folder create complete \n",
            "\n",
            "Epoch 1/3\n",
            "  2/180 [..............................] - ETA: 55:49 - loss: 7.7681 - pearson_correlation: 0.1544   "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1XVcK6KaHuzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs(history, 'pearson_correlation')"
      ],
      "metadata": {
        "id": "vd_2p_qQFJJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "id": "sAErsT9UFKTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KorSTS 테스트 데이터"
      ],
      "metadata": {
        "id": "gL1V1KrmFLdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Test dataset\n",
        "TEST_STS_DF = os.path.join(DATA_IN_PATH, 'KorSTS', 'sts-test.tsv')\n",
        "\n",
        "test_data = pd.read_csv(TEST_STS_DF, header=0, delimiter = '\\t', quoting = 3)\n",
        "test_data.head()"
      ],
      "metadata": {
        "id": "otS2kVWxFOi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set도 똑같은 방법으로 구성한다.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "data_labels = []\n",
        "\n",
        "for sent1, sent2, score in test_data[['sentence1', 'sentence2', 'score']].values:\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer_v2(clean_text(sent1), clean_text(sent2), MAX_LEN)\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        data_labels.append(score)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(sent1, sent2)\n",
        "        pass\n",
        "    \n",
        "test_input_ids = np.array(input_ids, dtype=int)\n",
        "test_attention_masks = np.array(attention_masks, dtype=int)\n",
        "test_type_ids = np.array(token_type_ids, dtype=int)\n",
        "test_inputs = (test_input_ids, test_attention_masks, test_type_ids)\n",
        "test_data_labels = np.array(data_labels)"
      ],
      "metadata": {
        "id": "fg4-5yRqFTKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"# sents: {}, # labels: {}\".format(len(test_input_ids), len(test_data_labels)))"
      ],
      "metadata": {
        "id": "U5CciVdQFXcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_model.load_weights(checkpoint_path)\n",
        "\n",
        "results = regression_model.evaluate(test_inputs, test_data_labels, batch_size=512)\n",
        "print(\"test loss, test pearson correlation: \", results)"
      ],
      "metadata": {
        "id": "NF2R8iNAFYUD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "KorSTS_finetuning.ipynb",
      "provenance": [],
      "mount_file_id": "16ZB5mYMOVVunB7tVpsDBcjxdVHrMm3Eh",
      "authorship_tag": "ABX9TyO0lf/piTW59rIsnV/hWlnF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}